{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd13873-9ec7-431b-be81-49998e416c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuche\\anaconda3\\envs\\waterforecast\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df7d093-b204-4ac0-a921-0927751d6342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1\n",
      "CUDA version: 11.3\n",
      "CUDA is available: True\n",
      "Number of GPUs available: 1\n",
      "GPU Properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2060', major=7, minor=5, total_memory=6143MB, multi_processor_count=30)\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"GPU Properties:\", torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c333a9fc-36e6-46e6-860a-d4ebbddb149b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>year</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue_date</th>\n",
       "      <th>volume_scaled</th>\n",
       "      <th>usgs_rw</th>\n",
       "      <th>mnf_scaled</th>\n",
       "      <th>mnf_rw</th>\n",
       "      <th>pred_mar</th>\n",
       "      <th>pred_apr</th>\n",
       "      <th>pred_may</th>\n",
       "      <th>pred_jun</th>\n",
       "      <th>pred_jul</th>\n",
       "      <th>mjo_rw</th>\n",
       "      <th>nino_rw</th>\n",
       "      <th>oni_rw</th>\n",
       "      <th>pdo_rw</th>\n",
       "      <th>pna_rw</th>\n",
       "      <th>soi_rw</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american_river_folsom_lake</td>\n",
       "      <td>1990</td>\n",
       "      <td>532.100</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>0.085628</td>\n",
       "      <td>[0.0028622252420492066, 0.002893983947030845, ...</td>\n",
       "      <td>0.097819</td>\n",
       "      <td>[0.03172846549634365, 0.04688166447469881, 0.0...</td>\n",
       "      <td>0.114502</td>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.084614</td>\n",
       "      <td>0.082711</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>[-0.5, -0.38, 0.47, 0.73, 1.32, -0.03, -0.64, ...</td>\n",
       "      <td>[-0.48, -0.23, 0.0, -0.13, -1.25, -1.15, -0.7,...</td>\n",
       "      <td>[-1.85, -1.69, -1.43, -1.08, -0.83, -0.58, -0....</td>\n",
       "      <td>[-1.24, -1.45, -1.24, -0.32, -0.09, 0.11, 0.56...</td>\n",
       "      <td>[-0.72, -1.06, -1.3, -0.54, -0.14, -0.63, -0.1...</td>\n",
       "      <td>[1.5, 1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american_river_folsom_lake</td>\n",
       "      <td>1990</td>\n",
       "      <td>532.100</td>\n",
       "      <td>1990-01-08</td>\n",
       "      <td>0.085628</td>\n",
       "      <td>[0.0027386981453826754, 0.0026116633254561206,...</td>\n",
       "      <td>0.097819</td>\n",
       "      <td>[0.03172846549634365, 0.04688166447469881, 0.0...</td>\n",
       "      <td>0.114502</td>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.084614</td>\n",
       "      <td>0.082711</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>[0.47, 0.73, 1.32, -0.03, -0.64, -1.22, -0.63,...</td>\n",
       "      <td>[-0.48, -0.23, 0.0, -0.13, -1.25, -1.15, -0.7,...</td>\n",
       "      <td>[-1.85, -1.69, -1.43, -1.08, -0.83, -0.58, -0....</td>\n",
       "      <td>[-1.24, -1.45, -1.24, -0.32, -0.09, 0.11, 0.56...</td>\n",
       "      <td>[-0.72, -1.06, -1.3, -0.54, -0.14, -0.63, -0.1...</td>\n",
       "      <td>[1.5, 1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>american_river_folsom_lake</td>\n",
       "      <td>1990</td>\n",
       "      <td>532.100</td>\n",
       "      <td>1990-01-15</td>\n",
       "      <td>0.085628</td>\n",
       "      <td>[0.0022214959691747344, 0.0021420992067206373,...</td>\n",
       "      <td>0.097819</td>\n",
       "      <td>[0.03172846549634365, 0.04688166447469881, 0.0...</td>\n",
       "      <td>0.114502</td>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.084614</td>\n",
       "      <td>0.082711</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>[0.73, 1.32, -0.03, -0.64, -1.22, -0.63, 0.46,...</td>\n",
       "      <td>[-0.48, -0.23, 0.0, -0.13, -1.25, -1.15, -0.7,...</td>\n",
       "      <td>[-1.85, -1.69, -1.43, -1.08, -0.83, -0.58, -0....</td>\n",
       "      <td>[-1.24, -1.45, -1.24, -0.32, -0.09, 0.11, 0.56...</td>\n",
       "      <td>[-0.72, -1.06, -1.3, -0.54, -0.14, -0.63, -0.1...</td>\n",
       "      <td>[1.5, 1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american_river_folsom_lake</td>\n",
       "      <td>1990</td>\n",
       "      <td>532.100</td>\n",
       "      <td>1990-01-22</td>\n",
       "      <td>0.085628</td>\n",
       "      <td>[0.0018314828817957956, 0.0018156035293049762,...</td>\n",
       "      <td>0.097819</td>\n",
       "      <td>[0.03172846549634365, 0.04688166447469881, 0.0...</td>\n",
       "      <td>0.114502</td>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.084614</td>\n",
       "      <td>0.082711</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>[1.32, -0.03, -0.64, -1.22, -0.63, 0.46, 0.56,...</td>\n",
       "      <td>[-0.48, -0.23, 0.0, -0.13, -1.25, -1.15, -0.7,...</td>\n",
       "      <td>[-1.85, -1.69, -1.43, -1.08, -0.83, -0.58, -0....</td>\n",
       "      <td>[-1.24, -1.45, -1.24, -0.32, -0.09, 0.11, 0.56...</td>\n",
       "      <td>[-0.72, -1.06, -1.3, -0.54, -0.14, -0.63, -0.1...</td>\n",
       "      <td>[1.5, 1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american_river_folsom_lake</td>\n",
       "      <td>1990</td>\n",
       "      <td>532.100</td>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>0.085628</td>\n",
       "      <td>[0.0015774132419426855, 0.0015003400544524373,...</td>\n",
       "      <td>0.061772</td>\n",
       "      <td>[0.04688166447469881, 0.057554633545957676, 0....</td>\n",
       "      <td>0.114502</td>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.084614</td>\n",
       "      <td>0.082711</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>[-0.64, -1.22, -0.63, 0.46, 0.56, -0.31, -0.87...</td>\n",
       "      <td>[-0.23, 0.0, -0.13, -1.25, -1.15, -0.7, -0.51,...</td>\n",
       "      <td>[-1.69, -1.43, -1.08, -0.83, -0.58, -0.4, -0.3...</td>\n",
       "      <td>[-1.45, -1.24, -0.32, -0.09, 0.11, 0.56, -0.44...</td>\n",
       "      <td>[-1.06, -1.3, -0.54, -0.14, -0.63, -0.18, -0.2...</td>\n",
       "      <td>[1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5, 0.8,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17467</th>\n",
       "      <td>yampa_r_nr_maybell</td>\n",
       "      <td>2022</td>\n",
       "      <td>680.923</td>\n",
       "      <td>2022-06-22</td>\n",
       "      <td>0.325856</td>\n",
       "      <td>[0.2350909063075659, 0.19937133555475045, 0.17...</td>\n",
       "      <td>0.155663</td>\n",
       "      <td>[0.017243066692590858, 0.05901254930924646, 0....</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.059013</td>\n",
       "      <td>0.204278</td>\n",
       "      <td>0.155663</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[0.74, 0.26, -0.84, 0.42, 0.34, -0.87, -0.7, -...</td>\n",
       "      <td>[-0.05, 0.25, -0.01, 0.18, -0.29, -0.81, -1.53...</td>\n",
       "      <td>[-0.48, -0.38, -0.4, -0.49, -0.67, -0.81, -0.9...</td>\n",
       "      <td>[-1.81, -1.96, -0.94, -1.96, -3.11, -2.75, -2....</td>\n",
       "      <td>[0.67, 0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1....</td>\n",
       "      <td>[0.4, 1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17468</th>\n",
       "      <td>yampa_r_nr_maybell</td>\n",
       "      <td>2022</td>\n",
       "      <td>680.923</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>0.325856</td>\n",
       "      <td>[0.22335447591735513, 0.19222742140418736, 0.1...</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[0.05901254930924646, 0.20427827444061816, 0.1...</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.059013</td>\n",
       "      <td>0.204278</td>\n",
       "      <td>0.155663</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[0.26, -0.84, 0.42, 0.34, -0.87, -0.7, -0.61, ...</td>\n",
       "      <td>[0.25, -0.01, 0.18, -0.29, -0.81, -1.53, -0.7,...</td>\n",
       "      <td>[-0.38, -0.4, -0.49, -0.67, -0.81, -0.98, -0.9...</td>\n",
       "      <td>[-1.96, -0.94, -1.96, -3.11, -2.75, -2.71, -2....</td>\n",
       "      <td>[0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1.01, 0....</td>\n",
       "      <td>[1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, 1.8, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17469</th>\n",
       "      <td>yampa_r_nr_maybell</td>\n",
       "      <td>2022</td>\n",
       "      <td>680.923</td>\n",
       "      <td>2022-07-08</td>\n",
       "      <td>0.325856</td>\n",
       "      <td>[0.3197973169499569, 0.32592067193615387, 0.31...</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[0.05901254930924646, 0.20427827444061816, 0.1...</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.059013</td>\n",
       "      <td>0.204278</td>\n",
       "      <td>0.155663</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[0.42, 0.34, -0.87, -0.7, -0.61, -1.27, -1.17,...</td>\n",
       "      <td>[0.25, -0.01, 0.18, -0.29, -0.81, -1.53, -0.7,...</td>\n",
       "      <td>[-0.38, -0.4, -0.49, -0.67, -0.81, -0.98, -0.9...</td>\n",
       "      <td>[-1.96, -0.94, -1.96, -3.11, -2.75, -2.71, -2....</td>\n",
       "      <td>[0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1.01, 0....</td>\n",
       "      <td>[1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, 1.8, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17470</th>\n",
       "      <td>yampa_r_nr_maybell</td>\n",
       "      <td>2022</td>\n",
       "      <td>680.923</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>0.325856</td>\n",
       "      <td>[0.25550208959488907, 0.18865546432890581, 0.1...</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[0.05901254930924646, 0.20427827444061816, 0.1...</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.059013</td>\n",
       "      <td>0.204278</td>\n",
       "      <td>0.155663</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[0.34, -0.87, -0.7, -0.61, -1.27, -1.17, 0.35,...</td>\n",
       "      <td>[0.25, -0.01, 0.18, -0.29, -0.81, -1.53, -0.7,...</td>\n",
       "      <td>[-0.38, -0.4, -0.49, -0.67, -0.81, -0.98, -0.9...</td>\n",
       "      <td>[-1.96, -0.94, -1.96, -3.11, -2.75, -2.71, -2....</td>\n",
       "      <td>[0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1.01, 0....</td>\n",
       "      <td>[1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, 1.8, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17471</th>\n",
       "      <td>yampa_r_nr_maybell</td>\n",
       "      <td>2022</td>\n",
       "      <td>680.923</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>0.325856</td>\n",
       "      <td>[0.1350761081996826, 0.12027800031637334, 0.10...</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[0.05901254930924646, 0.20427827444061816, 0.1...</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.059013</td>\n",
       "      <td>0.204278</td>\n",
       "      <td>0.155663</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>[-0.7, -0.61, -1.27, -1.17, 0.35, -1.39, -1.26...</td>\n",
       "      <td>[0.25, -0.01, 0.18, -0.29, -0.81, -1.53, -0.7,...</td>\n",
       "      <td>[-0.38, -0.4, -0.49, -0.67, -0.81, -0.98, -0.9...</td>\n",
       "      <td>[-1.96, -0.94, -1.96, -3.11, -2.75, -2.71, -2....</td>\n",
       "      <td>[0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1.01, 0....</td>\n",
       "      <td>[1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, 1.8, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17472 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          site_id  year   volume issue_date  volume_scaled  \\\n",
       "0      american_river_folsom_lake  1990  532.100 1990-01-01       0.085628   \n",
       "1      american_river_folsom_lake  1990  532.100 1990-01-08       0.085628   \n",
       "2      american_river_folsom_lake  1990  532.100 1990-01-15       0.085628   \n",
       "3      american_river_folsom_lake  1990  532.100 1990-01-22       0.085628   \n",
       "4      american_river_folsom_lake  1990  532.100 1990-02-01       0.085628   \n",
       "...                           ...   ...      ...        ...            ...   \n",
       "17467          yampa_r_nr_maybell  2022  680.923 2022-06-22       0.325856   \n",
       "17468          yampa_r_nr_maybell  2022  680.923 2022-07-01       0.325856   \n",
       "17469          yampa_r_nr_maybell  2022  680.923 2022-07-08       0.325856   \n",
       "17470          yampa_r_nr_maybell  2022  680.923 2022-07-15       0.325856   \n",
       "17471          yampa_r_nr_maybell  2022  680.923 2022-07-22       0.325856   \n",
       "\n",
       "                                                 usgs_rw  mnf_scaled  \\\n",
       "0      [0.0028622252420492066, 0.002893983947030845, ...    0.097819   \n",
       "1      [0.0027386981453826754, 0.0026116633254561206,...    0.097819   \n",
       "2      [0.0022214959691747344, 0.0021420992067206373,...    0.097819   \n",
       "3      [0.0018314828817957956, 0.0018156035293049762,...    0.097819   \n",
       "4      [0.0015774132419426855, 0.0015003400544524373,...    0.061772   \n",
       "...                                                  ...         ...   \n",
       "17467  [0.2350909063075659, 0.19937133555475045, 0.17...    0.155663   \n",
       "17468  [0.22335447591735513, 0.19222742140418736, 0.1...    0.025318   \n",
       "17469  [0.3197973169499569, 0.32592067193615387, 0.31...    0.025318   \n",
       "17470  [0.25550208959488907, 0.18865546432890581, 0.1...    0.025318   \n",
       "17471  [0.1350761081996826, 0.12027800031637334, 0.10...    0.025318   \n",
       "\n",
       "                                                  mnf_rw  pred_mar  pred_apr  \\\n",
       "0      [0.03172846549634365, 0.04688166447469881, 0.0...  0.114502  0.125828   \n",
       "1      [0.03172846549634365, 0.04688166447469881, 0.0...  0.114502  0.125828   \n",
       "2      [0.03172846549634365, 0.04688166447469881, 0.0...  0.114502  0.125828   \n",
       "3      [0.03172846549634365, 0.04688166447469881, 0.0...  0.114502  0.125828   \n",
       "4      [0.04688166447469881, 0.057554633545957676, 0....  0.114502  0.125828   \n",
       "...                                                  ...       ...       ...   \n",
       "17467  [0.017243066692590858, 0.05901254930924646, 0....  0.017243  0.059013   \n",
       "17468  [0.05901254930924646, 0.20427827444061816, 0.1...  0.017243  0.059013   \n",
       "17469  [0.05901254930924646, 0.20427827444061816, 0.1...  0.017243  0.059013   \n",
       "17470  [0.05901254930924646, 0.20427827444061816, 0.1...  0.017243  0.059013   \n",
       "17471  [0.05901254930924646, 0.20427827444061816, 0.1...  0.017243  0.059013   \n",
       "\n",
       "       pred_may  pred_jun  pred_jul  \\\n",
       "0      0.084614  0.082711  0.031674   \n",
       "1      0.084614  0.082711  0.031674   \n",
       "2      0.084614  0.082711  0.031674   \n",
       "3      0.084614  0.082711  0.031674   \n",
       "4      0.084614  0.082711  0.031674   \n",
       "...         ...       ...       ...   \n",
       "17467  0.204278  0.155663  0.025318   \n",
       "17468  0.204278  0.155663  0.025318   \n",
       "17469  0.204278  0.155663  0.025318   \n",
       "17470  0.204278  0.155663  0.025318   \n",
       "17471  0.204278  0.155663  0.025318   \n",
       "\n",
       "                                                  mjo_rw  \\\n",
       "0      [-0.5, -0.38, 0.47, 0.73, 1.32, -0.03, -0.64, ...   \n",
       "1      [0.47, 0.73, 1.32, -0.03, -0.64, -1.22, -0.63,...   \n",
       "2      [0.73, 1.32, -0.03, -0.64, -1.22, -0.63, 0.46,...   \n",
       "3      [1.32, -0.03, -0.64, -1.22, -0.63, 0.46, 0.56,...   \n",
       "4      [-0.64, -1.22, -0.63, 0.46, 0.56, -0.31, -0.87...   \n",
       "...                                                  ...   \n",
       "17467  [0.74, 0.26, -0.84, 0.42, 0.34, -0.87, -0.7, -...   \n",
       "17468  [0.26, -0.84, 0.42, 0.34, -0.87, -0.7, -0.61, ...   \n",
       "17469  [0.42, 0.34, -0.87, -0.7, -0.61, -1.27, -1.17,...   \n",
       "17470  [0.34, -0.87, -0.7, -0.61, -1.27, -1.17, 0.35,...   \n",
       "17471  [-0.7, -0.61, -1.27, -1.17, 0.35, -1.39, -1.26...   \n",
       "\n",
       "                                                 nino_rw  \\\n",
       "0      [-0.48, -0.23, 0.0, -0.13, -1.25, -1.15, -0.7,...   \n",
       "1      [-0.48, -0.23, 0.0, -0.13, -1.25, -1.15, -0.7,...   \n",
       "2      [-0.48, -0.23, 0.0, -0.13, -1.25, -1.15, -0.7,...   \n",
       "3      [-0.48, -0.23, 0.0, -0.13, -1.25, -1.15, -0.7,...   \n",
       "4      [-0.23, 0.0, -0.13, -1.25, -1.15, -0.7, -0.51,...   \n",
       "...                                                  ...   \n",
       "17467  [-0.05, 0.25, -0.01, 0.18, -0.29, -0.81, -1.53...   \n",
       "17468  [0.25, -0.01, 0.18, -0.29, -0.81, -1.53, -0.7,...   \n",
       "17469  [0.25, -0.01, 0.18, -0.29, -0.81, -1.53, -0.7,...   \n",
       "17470  [0.25, -0.01, 0.18, -0.29, -0.81, -1.53, -0.7,...   \n",
       "17471  [0.25, -0.01, 0.18, -0.29, -0.81, -1.53, -0.7,...   \n",
       "\n",
       "                                                  oni_rw  \\\n",
       "0      [-1.85, -1.69, -1.43, -1.08, -0.83, -0.58, -0....   \n",
       "1      [-1.85, -1.69, -1.43, -1.08, -0.83, -0.58, -0....   \n",
       "2      [-1.85, -1.69, -1.43, -1.08, -0.83, -0.58, -0....   \n",
       "3      [-1.85, -1.69, -1.43, -1.08, -0.83, -0.58, -0....   \n",
       "4      [-1.69, -1.43, -1.08, -0.83, -0.58, -0.4, -0.3...   \n",
       "...                                                  ...   \n",
       "17467  [-0.48, -0.38, -0.4, -0.49, -0.67, -0.81, -0.9...   \n",
       "17468  [-0.38, -0.4, -0.49, -0.67, -0.81, -0.98, -0.9...   \n",
       "17469  [-0.38, -0.4, -0.49, -0.67, -0.81, -0.98, -0.9...   \n",
       "17470  [-0.38, -0.4, -0.49, -0.67, -0.81, -0.98, -0.9...   \n",
       "17471  [-0.38, -0.4, -0.49, -0.67, -0.81, -0.98, -0.9...   \n",
       "\n",
       "                                                  pdo_rw  \\\n",
       "0      [-1.24, -1.45, -1.24, -0.32, -0.09, 0.11, 0.56...   \n",
       "1      [-1.24, -1.45, -1.24, -0.32, -0.09, 0.11, 0.56...   \n",
       "2      [-1.24, -1.45, -1.24, -0.32, -0.09, 0.11, 0.56...   \n",
       "3      [-1.24, -1.45, -1.24, -0.32, -0.09, 0.11, 0.56...   \n",
       "4      [-1.45, -1.24, -0.32, -0.09, 0.11, 0.56, -0.44...   \n",
       "...                                                  ...   \n",
       "17467  [-1.81, -1.96, -0.94, -1.96, -3.11, -2.75, -2....   \n",
       "17468  [-1.96, -0.94, -1.96, -3.11, -2.75, -2.71, -2....   \n",
       "17469  [-1.96, -0.94, -1.96, -3.11, -2.75, -2.71, -2....   \n",
       "17470  [-1.96, -0.94, -1.96, -3.11, -2.75, -2.71, -2....   \n",
       "17471  [-1.96, -0.94, -1.96, -3.11, -2.75, -2.71, -2....   \n",
       "\n",
       "                                                  pna_rw  \\\n",
       "0      [-0.72, -1.06, -1.3, -0.54, -0.14, -0.63, -0.1...   \n",
       "1      [-0.72, -1.06, -1.3, -0.54, -0.14, -0.63, -0.1...   \n",
       "2      [-0.72, -1.06, -1.3, -0.54, -0.14, -0.63, -0.1...   \n",
       "3      [-0.72, -1.06, -1.3, -0.54, -0.14, -0.63, -0.1...   \n",
       "4      [-1.06, -1.3, -0.54, -0.14, -0.63, -0.18, -0.2...   \n",
       "...                                                  ...   \n",
       "17467  [0.67, 0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1....   \n",
       "17468  [0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1.01, 0....   \n",
       "17469  [0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1.01, 0....   \n",
       "17470  [0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1.01, 0....   \n",
       "17471  [0.56, 0.95, 0.44, 1.13, 0.72, -2.56, 1.01, 0....   \n",
       "\n",
       "                                                  soi_rw  month  \n",
       "0      [1.5, 1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5,...      1  \n",
       "1      [1.5, 1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5,...      1  \n",
       "2      [1.5, 1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5,...      1  \n",
       "3      [1.5, 1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5,...      1  \n",
       "4      [1.2, 1.1, 1.6, 1.2, 0.7, 0.9, -0.3, 0.5, 0.8,...      2  \n",
       "...                                                  ...    ...  \n",
       "17467  [0.4, 1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, ...      6  \n",
       "17468  [1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, 1.8, ...      7  \n",
       "17469  [1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, 1.8, ...      7  \n",
       "17470  [1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, 1.8, ...      7  \n",
       "17471  [1.4, 0.6, 0.8, 0.7, 1.0, 1.5, 0.5, 1.1, 1.8, ...      7  \n",
       "\n",
       "[17472 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_processed.csv')\n",
    "data['issue_date'] = pd.to_datetime(data['issue_date'])\n",
    "data['month'] = data['issue_date'].dt.month\n",
    "data['year'] = data['issue_date'].dt.year\n",
    "cols = ['usgs_rw','mnf_rw','mjo_rw','nino_rw','oni_rw','pdo_rw','pna_rw','soi_rw']\n",
    "def string_to_float_list(s):\n",
    "    items = s.strip('[]').split(',')\n",
    "    return [float(item.strip()) for item in items if item]\n",
    "for col in cols:\n",
    "    data[col] = data[col].apply(string_to_float_list)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f6c9c-7665-4da3-b289-4d0ba005dffa",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8397364-b18b-4954-890e-7b0c75bc6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLSTM(nn.Module):\n",
    "    def __init__(self, months, dropout_rate = 0.2):\n",
    "        super(MultiLSTM, self).__init__()\n",
    "        # LSTM layers for each input\n",
    "        self.lstm_usgs = nn.LSTM(30, 32, 2, batch_first = True)\n",
    "        self.lstm_mnf = nn.LSTM(3, 32, batch_first = True)\n",
    "        self.lstm_mjo = nn.LSTM(18, 32, 2, batch_first = True)\n",
    "        self.lstm_nino = nn.LSTM(12, 32, batch_first = True)\n",
    "        self.lstm_oni = nn.LSTM(12, 32, batch_first = True)\n",
    "        self.lstm_pdo = nn.LSTM(12, 32, batch_first = True)\n",
    "        self.lstm_pna = nn.LSTM(12, 32, batch_first = True)\n",
    "        self.lstm_soi = nn.LSTM(12, 32, batch_first = True)\n",
    "        self.month = nn.Embedding(months+1, 32)\n",
    "\n",
    "        # Total concat size\n",
    "        concat_size = 32*9\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Final dense layers after concatenation\n",
    "        self.fcc_1 = nn.Linear(concat_size, 128)\n",
    "        self.fcc_2 = nn.Linear(128, 128)\n",
    "        self.fcc_3 = nn.Linear(128, 64)\n",
    "        self.fcc_4 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, usgs, mnf, mjo, nino, oni, pdo, pna, soi, month):\n",
    "        # Process each LSTM Input\n",
    "        usgs_out, _ = self.lstm_usgs(usgs)\n",
    "        mnf_out, _ = self.lstm_mnf(mnf)\n",
    "        mjo_out, _ = self.lstm_mjo(mjo)\n",
    "        nino_out, _ = self.lstm_nino(nino)\n",
    "        oni_out, _ = self.lstm_oni(oni)\n",
    "        pdo_out, _ = self.lstm_pdo(pdo)\n",
    "        pna_out, _ = self.lstm_pna(pna)\n",
    "        soi_out, _ = self.lstm_soi(soi)\n",
    "        month_out = self.month(month)\n",
    "        # Combine all outputs\n",
    "        combined = torch.cat([usgs_out, mnf_out, mjo_out, nino_out, oni_out, pdo_out, pna_out, soi_out, month_out], dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        # Final dense layers and output\n",
    "        # Pass through fully connected layers\n",
    "        # x = torch.relu(self.fcc_1(combined))\n",
    "        x = self.fcc_1(combined)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fcc_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fcc_3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fcc_4(x))\n",
    "\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "    def enable_dropout(self):\n",
    "        \"\"\"enable montecarlo dropout\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "                            Default: 5\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                                Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2803fb35-8072-43dd-bf4f-6d523d70e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(preds, target, quantiles = [0.10,0.50,0.90]):\n",
    "    assert len(quantiles) == preds.shape[1]\n",
    "    loss = 0\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - preds[:, i]\n",
    "        loss += torch.max((q-1) * errors, q * errors).mean()\n",
    "    return loss / len(quantiles)\n",
    "def train_LSTM(df, pred_month, output_col, num_epochs = 200, model_name = 'model.pth'):\n",
    "    filtered_df = df[df['month']<=pred_month]\n",
    "    usgs_tensor = torch.tensor(filtered_df['usgs_rw'].tolist()).float()\n",
    "    mnf_tensor = torch.tensor(filtered_df['mnf_rw'].tolist()).float()\n",
    "    mjo_tensor = torch.tensor(filtered_df['mjo_rw'].tolist()).float()\n",
    "    nino_tensor = torch.tensor(filtered_df['nino_rw'].tolist()).float()\n",
    "    oni_tensor = torch.tensor(filtered_df['oni_rw'].tolist()).float()\n",
    "    pdo_tensor = torch.tensor(filtered_df['pdo_rw'].tolist()).float()\n",
    "    pna_tensor = torch.tensor(filtered_df['pna_rw'].tolist()).float()\n",
    "    soi_tensor = torch.tensor(filtered_df['soi_rw'].tolist()).float()\n",
    "    month_tensor = torch.tensor(filtered_df['month'].tolist(), dtype = torch.float32).long()\n",
    "    target_tensor = torch.tensor(filtered_df[output_col].tolist()).float()\n",
    "    dataset = TensorDataset(usgs_tensor, mnf_tensor, mjo_tensor, nino_tensor, oni_tensor, pdo_tensor, pna_tensor, soi_tensor, month_tensor, target_tensor)\n",
    "    \n",
    "    val_size = int(len(dataset)*0.1)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model = MultiLSTM(pred_month)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.0001)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            usgs, mnf, mjo, nino, oni, pdo, pna, soi, month, target = batch\n",
    "    \n",
    "            # Move data to GPU\n",
    "            if torch.cuda.is_available():\n",
    "                usgs, mnf, mjo, nino, oni, pdo, pna, soi, month, target = [x.cuda() for x in [usgs, mnf, mjo, nino, oni, pdo, pna, soi, month, target]]\n",
    "    \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(usgs, mnf, mjo, nino, oni, pdo, pna, soi, month)\n",
    "    \n",
    "            # Compute loss\n",
    "            loss = quantile_loss(outputs, target)\n",
    "    \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        model.eval() # Set model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                usgs, mnf, mjo, nino, oni, pdo, pna, soi, month, target = batch\n",
    "                if torch.cuda.is_available():\n",
    "                    usgs, mnf, mjo, nino, oni, pdo, pna, soi, month, target = [x.cuda() for x in [usgs, mnf, mjo, nino, oni, pdo, pna, soi, month, target]]\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = model(usgs, mnf, mjo, nino, oni, pdo, pna, soi, month)\n",
    "    \n",
    "                # Calculate loss\n",
    "                val_loss = quantile_loss(outputs, target)\n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "        early_stopping(avg_val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c5ab90-2ae3-41f8-9793-c41c37249b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 0.0152, Validation Loss: 0.0117\n",
      "Epoch [2/200], Training Loss: 0.0139, Validation Loss: 0.0102\n",
      "Epoch [3/200], Training Loss: 0.0115, Validation Loss: 0.0082\n",
      "Epoch [4/200], Training Loss: 0.0104, Validation Loss: 0.0090\n",
      "Epoch [5/200], Training Loss: 0.0104, Validation Loss: 0.0076\n",
      "Epoch [6/200], Training Loss: 0.0100, Validation Loss: 0.0078\n",
      "Epoch [7/200], Training Loss: 0.0097, Validation Loss: 0.0074\n",
      "Epoch [8/200], Training Loss: 0.0096, Validation Loss: 0.0074\n",
      "Epoch [9/200], Training Loss: 0.0092, Validation Loss: 0.0071\n",
      "Epoch [10/200], Training Loss: 0.0090, Validation Loss: 0.0071\n",
      "Epoch [11/200], Training Loss: 0.0088, Validation Loss: 0.0063\n",
      "Epoch [12/200], Training Loss: 0.0088, Validation Loss: 0.0063\n",
      "Epoch [13/200], Training Loss: 0.0086, Validation Loss: 0.0068\n",
      "Epoch [14/200], Training Loss: 0.0085, Validation Loss: 0.0062\n",
      "Epoch [15/200], Training Loss: 0.0084, Validation Loss: 0.0064\n",
      "Epoch [16/200], Training Loss: 0.0080, Validation Loss: 0.0063\n",
      "Epoch [17/200], Training Loss: 0.0082, Validation Loss: 0.0061\n",
      "Epoch [18/200], Training Loss: 0.0080, Validation Loss: 0.0060\n",
      "Epoch [19/200], Training Loss: 0.0080, Validation Loss: 0.0065\n",
      "Epoch [20/200], Training Loss: 0.0079, Validation Loss: 0.0064\n",
      "Epoch [21/200], Training Loss: 0.0076, Validation Loss: 0.0060\n",
      "Epoch [22/200], Training Loss: 0.0076, Validation Loss: 0.0061\n",
      "Epoch [23/200], Training Loss: 0.0073, Validation Loss: 0.0058\n",
      "Epoch [24/200], Training Loss: 0.0070, Validation Loss: 0.0056\n",
      "Epoch [25/200], Training Loss: 0.0071, Validation Loss: 0.0056\n",
      "Epoch [26/200], Training Loss: 0.0070, Validation Loss: 0.0057\n",
      "Epoch [27/200], Training Loss: 0.0067, Validation Loss: 0.0057\n",
      "Epoch [28/200], Training Loss: 0.0068, Validation Loss: 0.0061\n",
      "Epoch [29/200], Training Loss: 0.0071, Validation Loss: 0.0053\n",
      "Epoch [30/200], Training Loss: 0.0066, Validation Loss: 0.0055\n",
      "Epoch [31/200], Training Loss: 0.0064, Validation Loss: 0.0056\n",
      "Epoch [32/200], Training Loss: 0.0064, Validation Loss: 0.0054\n",
      "Epoch [33/200], Training Loss: 0.0065, Validation Loss: 0.0054\n",
      "Epoch [34/200], Training Loss: 0.0065, Validation Loss: 0.0054\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(data, 3, 'pred_mar', model_name = 'mar_modelth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc27a7d0-ddeb-4273-96ea-58ecc9751bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 0.0182, Validation Loss: 0.0148\n",
      "Epoch [2/200], Training Loss: 0.0141, Validation Loss: 0.0118\n",
      "Epoch [3/200], Training Loss: 0.0123, Validation Loss: 0.0117\n",
      "Epoch [4/200], Training Loss: 0.0118, Validation Loss: 0.0109\n",
      "Epoch [5/200], Training Loss: 0.0113, Validation Loss: 0.0106\n",
      "Epoch [6/200], Training Loss: 0.0111, Validation Loss: 0.0103\n",
      "Epoch [7/200], Training Loss: 0.0107, Validation Loss: 0.0100\n",
      "Epoch [8/200], Training Loss: 0.0106, Validation Loss: 0.0098\n",
      "Epoch [9/200], Training Loss: 0.0104, Validation Loss: 0.0102\n",
      "Epoch [10/200], Training Loss: 0.0103, Validation Loss: 0.0096\n",
      "Epoch [11/200], Training Loss: 0.0102, Validation Loss: 0.0097\n",
      "Epoch [12/200], Training Loss: 0.0101, Validation Loss: 0.0097\n",
      "Epoch [13/200], Training Loss: 0.0102, Validation Loss: 0.0097\n",
      "Epoch [14/200], Training Loss: 0.0100, Validation Loss: 0.0095\n",
      "Epoch [15/200], Training Loss: 0.0100, Validation Loss: 0.0094\n",
      "Epoch [16/200], Training Loss: 0.0100, Validation Loss: 0.0096\n",
      "Epoch [17/200], Training Loss: 0.0099, Validation Loss: 0.0096\n",
      "Epoch [18/200], Training Loss: 0.0099, Validation Loss: 0.0094\n",
      "Epoch [19/200], Training Loss: 0.0098, Validation Loss: 0.0092\n",
      "Epoch [20/200], Training Loss: 0.0097, Validation Loss: 0.0096\n",
      "Epoch [21/200], Training Loss: 0.0096, Validation Loss: 0.0095\n",
      "Epoch [22/200], Training Loss: 0.0096, Validation Loss: 0.0091\n",
      "Epoch [23/200], Training Loss: 0.0094, Validation Loss: 0.0091\n",
      "Epoch [24/200], Training Loss: 0.0095, Validation Loss: 0.0091\n",
      "Epoch [25/200], Training Loss: 0.0094, Validation Loss: 0.0090\n",
      "Epoch [26/200], Training Loss: 0.0094, Validation Loss: 0.0088\n",
      "Epoch [27/200], Training Loss: 0.0094, Validation Loss: 0.0090\n",
      "Epoch [28/200], Training Loss: 0.0092, Validation Loss: 0.0089\n",
      "Epoch [29/200], Training Loss: 0.0092, Validation Loss: 0.0089\n",
      "Epoch [30/200], Training Loss: 0.0091, Validation Loss: 0.0087\n",
      "Epoch [31/200], Training Loss: 0.0091, Validation Loss: 0.0086\n",
      "Epoch [32/200], Training Loss: 0.0090, Validation Loss: 0.0086\n",
      "Epoch [33/200], Training Loss: 0.0090, Validation Loss: 0.0085\n",
      "Epoch [34/200], Training Loss: 0.0090, Validation Loss: 0.0084\n",
      "Epoch [35/200], Training Loss: 0.0089, Validation Loss: 0.0085\n",
      "Epoch [36/200], Training Loss: 0.0090, Validation Loss: 0.0084\n",
      "Epoch [37/200], Training Loss: 0.0089, Validation Loss: 0.0084\n",
      "Epoch [38/200], Training Loss: 0.0088, Validation Loss: 0.0085\n",
      "Epoch [39/200], Training Loss: 0.0089, Validation Loss: 0.0082\n",
      "Epoch [40/200], Training Loss: 0.0087, Validation Loss: 0.0085\n",
      "Epoch [41/200], Training Loss: 0.0087, Validation Loss: 0.0083\n",
      "Epoch [42/200], Training Loss: 0.0088, Validation Loss: 0.0083\n",
      "Epoch [43/200], Training Loss: 0.0088, Validation Loss: 0.0083\n",
      "Epoch [44/200], Training Loss: 0.0086, Validation Loss: 0.0083\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(data, 4, 'pred_apr', model_name = 'apr_model_quant.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e63acf-fa5e-4eef-9697-d32498c4997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 0.0256, Validation Loss: 0.0201\n",
      "Epoch [2/200], Training Loss: 0.0199, Validation Loss: 0.0186\n",
      "Epoch [3/200], Training Loss: 0.0188, Validation Loss: 0.0183\n",
      "Epoch [4/200], Training Loss: 0.0183, Validation Loss: 0.0181\n",
      "Epoch [5/200], Training Loss: 0.0180, Validation Loss: 0.0177\n",
      "Epoch [6/200], Training Loss: 0.0177, Validation Loss: 0.0174\n",
      "Epoch [7/200], Training Loss: 0.0175, Validation Loss: 0.0177\n",
      "Epoch [8/200], Training Loss: 0.0174, Validation Loss: 0.0172\n",
      "Epoch [9/200], Training Loss: 0.0172, Validation Loss: 0.0173\n",
      "Epoch [10/200], Training Loss: 0.0172, Validation Loss: 0.0170\n",
      "Epoch [11/200], Training Loss: 0.0170, Validation Loss: 0.0169\n",
      "Epoch [12/200], Training Loss: 0.0169, Validation Loss: 0.0168\n",
      "Epoch [13/200], Training Loss: 0.0168, Validation Loss: 0.0171\n",
      "Epoch [14/200], Training Loss: 0.0167, Validation Loss: 0.0164\n",
      "Epoch [15/200], Training Loss: 0.0166, Validation Loss: 0.0163\n",
      "Epoch [16/200], Training Loss: 0.0164, Validation Loss: 0.0162\n",
      "Epoch [17/200], Training Loss: 0.0164, Validation Loss: 0.0163\n",
      "Epoch [18/200], Training Loss: 0.0162, Validation Loss: 0.0162\n",
      "Epoch [19/200], Training Loss: 0.0161, Validation Loss: 0.0157\n",
      "Epoch [20/200], Training Loss: 0.0160, Validation Loss: 0.0155\n",
      "Epoch [21/200], Training Loss: 0.0159, Validation Loss: 0.0153\n",
      "Epoch [22/200], Training Loss: 0.0157, Validation Loss: 0.0153\n",
      "Epoch [23/200], Training Loss: 0.0156, Validation Loss: 0.0153\n",
      "Epoch [24/200], Training Loss: 0.0155, Validation Loss: 0.0152\n",
      "Epoch [25/200], Training Loss: 0.0156, Validation Loss: 0.0150\n",
      "Epoch [26/200], Training Loss: 0.0154, Validation Loss: 0.0150\n",
      "Epoch [27/200], Training Loss: 0.0153, Validation Loss: 0.0148\n",
      "Epoch [28/200], Training Loss: 0.0152, Validation Loss: 0.0147\n",
      "Epoch [29/200], Training Loss: 0.0152, Validation Loss: 0.0149\n",
      "Epoch [30/200], Training Loss: 0.0151, Validation Loss: 0.0145\n",
      "Epoch [31/200], Training Loss: 0.0150, Validation Loss: 0.0146\n",
      "Epoch [32/200], Training Loss: 0.0151, Validation Loss: 0.0144\n",
      "Epoch [33/200], Training Loss: 0.0149, Validation Loss: 0.0146\n",
      "Epoch [34/200], Training Loss: 0.0149, Validation Loss: 0.0140\n",
      "Epoch [35/200], Training Loss: 0.0148, Validation Loss: 0.0142\n",
      "Epoch [36/200], Training Loss: 0.0148, Validation Loss: 0.0141\n",
      "Epoch [37/200], Training Loss: 0.0146, Validation Loss: 0.0139\n",
      "Epoch [38/200], Training Loss: 0.0146, Validation Loss: 0.0141\n",
      "Epoch [39/200], Training Loss: 0.0146, Validation Loss: 0.0137\n",
      "Epoch [40/200], Training Loss: 0.0145, Validation Loss: 0.0139\n",
      "Epoch [41/200], Training Loss: 0.0144, Validation Loss: 0.0138\n",
      "Epoch [42/200], Training Loss: 0.0143, Validation Loss: 0.0137\n",
      "Epoch [43/200], Training Loss: 0.0144, Validation Loss: 0.0138\n",
      "Epoch [44/200], Training Loss: 0.0144, Validation Loss: 0.0137\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(data, 5, 'pred_may', model_name = 'may_model_quant.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c1bc5b-473a-4255-8072-546a030fdbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 0.0290, Validation Loss: 0.0259\n",
      "Epoch [2/200], Training Loss: 0.0244, Validation Loss: 0.0236\n",
      "Epoch [3/200], Training Loss: 0.0226, Validation Loss: 0.0226\n",
      "Epoch [4/200], Training Loss: 0.0217, Validation Loss: 0.0209\n",
      "Epoch [5/200], Training Loss: 0.0210, Validation Loss: 0.0208\n",
      "Epoch [6/200], Training Loss: 0.0205, Validation Loss: 0.0201\n",
      "Epoch [7/200], Training Loss: 0.0202, Validation Loss: 0.0201\n",
      "Epoch [8/200], Training Loss: 0.0199, Validation Loss: 0.0196\n",
      "Epoch [9/200], Training Loss: 0.0198, Validation Loss: 0.0197\n",
      "Epoch [10/200], Training Loss: 0.0196, Validation Loss: 0.0196\n",
      "Epoch [11/200], Training Loss: 0.0193, Validation Loss: 0.0193\n",
      "Epoch [12/200], Training Loss: 0.0193, Validation Loss: 0.0195\n",
      "Epoch [13/200], Training Loss: 0.0190, Validation Loss: 0.0194\n",
      "Epoch [14/200], Training Loss: 0.0189, Validation Loss: 0.0192\n",
      "Epoch [15/200], Training Loss: 0.0189, Validation Loss: 0.0189\n",
      "Epoch [16/200], Training Loss: 0.0188, Validation Loss: 0.0187\n",
      "Epoch [17/200], Training Loss: 0.0186, Validation Loss: 0.0188\n",
      "Epoch [18/200], Training Loss: 0.0186, Validation Loss: 0.0185\n",
      "Epoch [19/200], Training Loss: 0.0184, Validation Loss: 0.0187\n",
      "Epoch [20/200], Training Loss: 0.0183, Validation Loss: 0.0185\n",
      "Epoch [21/200], Training Loss: 0.0183, Validation Loss: 0.0191\n",
      "Epoch [22/200], Training Loss: 0.0181, Validation Loss: 0.0182\n",
      "Epoch [23/200], Training Loss: 0.0181, Validation Loss: 0.0187\n",
      "Epoch [24/200], Training Loss: 0.0180, Validation Loss: 0.0180\n",
      "Epoch [25/200], Training Loss: 0.0179, Validation Loss: 0.0177\n",
      "Epoch [26/200], Training Loss: 0.0178, Validation Loss: 0.0176\n",
      "Epoch [27/200], Training Loss: 0.0179, Validation Loss: 0.0182\n",
      "Epoch [28/200], Training Loss: 0.0178, Validation Loss: 0.0178\n",
      "Epoch [29/200], Training Loss: 0.0175, Validation Loss: 0.0178\n",
      "Epoch [30/200], Training Loss: 0.0176, Validation Loss: 0.0173\n",
      "Epoch [31/200], Training Loss: 0.0175, Validation Loss: 0.0178\n",
      "Epoch [32/200], Training Loss: 0.0173, Validation Loss: 0.0174\n",
      "Epoch [33/200], Training Loss: 0.0173, Validation Loss: 0.0171\n",
      "Epoch [34/200], Training Loss: 0.0172, Validation Loss: 0.0176\n",
      "Epoch [35/200], Training Loss: 0.0171, Validation Loss: 0.0168\n",
      "Epoch [36/200], Training Loss: 0.0172, Validation Loss: 0.0169\n",
      "Epoch [37/200], Training Loss: 0.0170, Validation Loss: 0.0168\n",
      "Epoch [38/200], Training Loss: 0.0170, Validation Loss: 0.0173\n",
      "Epoch [39/200], Training Loss: 0.0169, Validation Loss: 0.0170\n",
      "Epoch [40/200], Training Loss: 0.0168, Validation Loss: 0.0170\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(data, 6, 'pred_jun', model_name = 'jun_model_quant.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4faa5b94-0a7f-4b7b-a309-69e0f91eb35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 0.0140, Validation Loss: 0.0128\n",
      "Epoch [2/200], Training Loss: 0.0124, Validation Loss: 0.0118\n",
      "Epoch [3/200], Training Loss: 0.0119, Validation Loss: 0.0112\n",
      "Epoch [4/200], Training Loss: 0.0116, Validation Loss: 0.0111\n",
      "Epoch [5/200], Training Loss: 0.0114, Validation Loss: 0.0107\n",
      "Epoch [6/200], Training Loss: 0.0112, Validation Loss: 0.0105\n",
      "Epoch [7/200], Training Loss: 0.0110, Validation Loss: 0.0103\n",
      "Epoch [8/200], Training Loss: 0.0108, Validation Loss: 0.0104\n",
      "Epoch [9/200], Training Loss: 0.0108, Validation Loss: 0.0102\n",
      "Epoch [10/200], Training Loss: 0.0107, Validation Loss: 0.0100\n",
      "Epoch [11/200], Training Loss: 0.0106, Validation Loss: 0.0100\n",
      "Epoch [12/200], Training Loss: 0.0105, Validation Loss: 0.0099\n",
      "Epoch [13/200], Training Loss: 0.0105, Validation Loss: 0.0099\n",
      "Epoch [14/200], Training Loss: 0.0103, Validation Loss: 0.0097\n",
      "Epoch [15/200], Training Loss: 0.0102, Validation Loss: 0.0097\n",
      "Epoch [16/200], Training Loss: 0.0102, Validation Loss: 0.0096\n",
      "Epoch [17/200], Training Loss: 0.0101, Validation Loss: 0.0095\n",
      "Epoch [18/200], Training Loss: 0.0101, Validation Loss: 0.0098\n",
      "Epoch [19/200], Training Loss: 0.0101, Validation Loss: 0.0094\n",
      "Epoch [20/200], Training Loss: 0.0099, Validation Loss: 0.0093\n",
      "Epoch [21/200], Training Loss: 0.0099, Validation Loss: 0.0092\n",
      "Epoch [22/200], Training Loss: 0.0099, Validation Loss: 0.0094\n",
      "Epoch [23/200], Training Loss: 0.0098, Validation Loss: 0.0092\n",
      "Epoch [24/200], Training Loss: 0.0097, Validation Loss: 0.0092\n",
      "Epoch [25/200], Training Loss: 0.0098, Validation Loss: 0.0094\n",
      "Epoch [26/200], Training Loss: 0.0096, Validation Loss: 0.0092\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(data, 7, 'pred_jul', model_name = 'jul_model_quant.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
